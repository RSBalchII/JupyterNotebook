{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a890d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c563f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Annotated, TypedDict, Optional\n",
    "import pandas as pd # Assuming pandas is a project dependency for data manipulation\n",
    "import logging # Use standard logging or integrate loguru directly if preferred\n",
    "\n",
    "# Import actual classes from your project\n",
    "from DataDistributor import DataDistributor\n",
    "from Constants import Constants # For API keys, database names etc.\n",
    "from RequestContext import RequestContext as ctx # For context-aware logging\n",
    "from Chain import LangChainAgent # Assuming LangChainAgent is the primary LLM wrapper\n",
    "# Assuming your project uses loguru and it's configured globally or via RequestContext\n",
    "# from Logging.ErrorLogger import logger # If you need direct access outside context\n",
    "\n",
    "# You will need to replace this with your actual LLM initialization\n",
    "# from langchain_core.language_models import BaseChatModel\n",
    "# from langchain_core.output_parsers import JsonOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage # Assuming LangChainAgent returns HumanMessage or similar\n",
    "\n",
    "# --- LangGraph Setup ---\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "# from langgraph.prebuilt import ToolExecutor # You will likely use this in your real implementation\n",
    "# from langchain.tools import BaseTool # Not directly used if DataDistributor methods are called directly\n",
    "\n",
    "# Placeholder for LIDA Manager - requires installation: pip install lida\n",
    "# And you'll need an API key for LIDA's text generation model\n",
    "from lida import Manager, TextGenerationConfig, DataConfig\n",
    "\n",
    "# Setup basic logging for notebook execution context\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea949bc9",
   "metadata": {},
   "source": [
    "## Tools (Implemented via DataDistributor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35092db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual tool implementations are within DataDistributor methods:\n",
    "# DataDistributor.vector_search\n",
    "# DataDistributor.parameter_search\n",
    "\n",
    "# The GraphBuilder will call these methods directly or via a wrapper\n",
    "# No separate tool classes like MockVectorSearchTool are needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8789eb",
   "metadata": {},
   "source": [
    "## Data Framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579077b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function transforms raw data fetched from the database\n",
    "# into a Pandas DataFrame, which LIDA requires.\n",
    "def data_to_dataframe(raw_data: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms raw data (list of dicts) into a Pandas DataFrame.\n",
    "    Assumes raw_data is already a list of dictionaries.\n",
    "    \"\"\"\n",
    "    print(\"Transforming data to DataFrame...\")\n",
    "    # Add logic to flatten or process data if needed based on your schema\n",
    "    # For simple list of dicts, direct conversion is fine.\n",
    "    if not isinstance(raw_data, list) or not all(isinstance(item, dict) for item in raw_data):\\\n",
    "        ctx.logger().error(\"data_to_dataframe received data not in expected list of dicts format.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49164d76",
   "metadata": {},
   "source": [
    "## LLM Classes (Implemented via LangChainAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual LLM interactions are handled by LangChainAgent.\n",
    "# We will initialize different instances or use methods of LangChainAgent\n",
    "# for different purposes (decision, tool calling prompt, summarization, answer generation).\n",
    "# Based on QueryHandler, LangChainAgent is initialized with an API key and optionally a summary_workflow flag.\n",
    "# We might need to add specific methods to LangChainAgent or QueryHandler\n",
    "# for decision making and tool call prompt generation if they don't exist.\n",
    "\n",
    "# For this refactor, we'll assume LangChainAgent can handle different prompts\n",
    "# based on how it's invoked or configured.\n",
    "\n",
    "# Placeholder for LLM initialization based on project's Constants\n",
    "def initialize_llm_agent(summary_workflow=False, model_name=''):\n",
    "    \"\"\"\n",
    "    Initializes a LangChainAgent using project's Constants.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_key = Constants._get_google_ai_key()\n",
    "        if not api_key:\n",
    "             ctx.logger().error(\"Google AI API key not found in Constants.\")\n",
    "             raise ValueError(\"Google AI API key not configured.\")\n",
    "\n",
    "        # You might need different model names based on the task (decision, generation, etc.)\n",
    "        # The model_name parameter in LangChainAgent needs to be aligned with available models.\n",
    "        # If LangChainAgent handles prompt routing internally, you might only need one instance.\n",
    "\n",
    "        return LangChainAgent(api_key=api_key, model_name=model_name, summary_workflow=summary_workflow)\n",
    "    except Exception as e:\n",
    "        ctx.logger().error(f\"Failed to initialize LangChainAgent: {e}\", eventid=\"tag_llm_init_fail\")\n",
    "        # Depending on error handling strategy, you might raise or return None\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048da3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "640f24f8",
   "metadata": {},
   "source": [
    "## State and Graph Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975dcedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for your add_messages function\n",
    "# This should align with how messages are handled in your project, likely in QueryHandler or similar.\n",
    "# If message history is managed externally and passed into the state, this function might be simplified or removed.\n",
    "# Assuming for now it just appends messages as in the original notebook.\n",
    "def add_messages(left: list, right: list):\n",
    "    \"\"\"\n",
    "    Append the new messages to the existing messages.\n",
    "    This is a placeholder and should be replaced with your project's actual logic for managing messages.\n",
    "    \"\"\"\n",
    "    left.extend(right)\n",
    "    return left\n",
    "\n",
    "# Update State TypedDict to match project's schema and include visualization fields\n",
    "# Align these fields with your project's actual state/context schema\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages] # Or however message history is tracked\n",
    "    history: str # Text representation of history, used in prompts\n",
    "    question: str # Current user question\n",
    "    # These splitting/needs_history fields are from the original notebook, keep if relevant\n",
    "    should_split: bool\n",
    "    split_questions: list\n",
    "    needs_history: bool\n",
    "    needs_context: bool\n",
    "    context: str # Fetched data converted to string for text workflow\n",
    "    raw_context_data: Optional[list] # Store raw data list of dicts for visualization\n",
    "    summarized_context: str # Summarized context for text answer\n",
    "    final_answer: str # Final text answer\n",
    "\n",
    "    # Fields for visualization workflow\n",
    "    visualization_request: Optional[dict] # Structured request from decision LLM\n",
    "    # visualization_data: Optional[list] # Renamed to raw_context_data for clarity\n",
    "    visualization_output: Optional[dict] # Vega-Lite spec or image data\n",
    "    visualization_error: Optional[str] # Error message if visualization fails\n",
    "\n",
    "class GraphBuilder:\n",
    "    def __init__(self):\\\n",
    "        # Initialize actual LLM models and DataDistributor\n",
    "        # Assuming different LangChainAgent instances are needed for different tasks,\\\n",
    "        # or your LangChainAgent/QueryHandler handles internal routing.\\\n",
    "        # If QueryHandler methods are used, initialize QueryHandler here.\\\n",
    "        from QueryHandler import QueryHandler # Import QueryHandler\n",
    "        self.query_handler = QueryHandler() # Use the project's QueryHandler\n",
    "\n",
    "        # Initialize LIDA Manager\n",
    "        try:\n",
    "            # Get LIDA API key using project's Constants or environment variables\n",
    "            # Assuming Constants can provide a generic API key or a specific LIDA one\n",
    "            lida_api_key = Constants._get_google_ai_key() # Or Constants.get_lida_api_key()\n",
    "            if not lida_api_key:\n",
    "                 # Fallback to environment variable if not in Constants\n",
    "                 lida_api_key = os.environ.get(\"GOOGLE_API_KEY\") # Or \"LIDA_API_KEY\"\n",
    "\n",
    "            if not lida_api_key:\n",
    "                 ctx.logger().error(\"LIDA API key not found in Constants or environment variables.\", eventid=\"tag_lida_key_missing\")\n",
    "                 # Handle this error appropriately - maybe disable viz or raise error\n",
    "                 self.lida = None # Disable LIDA if key is missing\n",
    "                 print(\"Warning: LIDA API key is not configured. Visualization may not work.\")\n",
    "            else:\n",
    "                self.lida = Manager(text_gen_config=TextGenerationConfig(\n",
    "                    model=\"gemini-flash\", # Or the model you intend to use for LIDA\n",
    "                    api_key=lida_api_key\n",
    "                ))\n",
    "\n",
    "        except ImportError:\n",
    "             ctx.logger().error(\"LIDA or its dependencies not installed.\", eventid=\"tag_lida_import_error\")\n",
    "             self.lida = None\n",
    "             print(\"Warning: LIDA library is not installed (pip install lida). Visualization will be disabled.\")\n",
    "        except Exception as e:\n",
    "            ctx.logger().error(f\"Failed to initialize LIDA Manager: {e}\", eventid=\"tag_lida_init_fail\")\n",
    "            self.lida = None\n",
    "            print(f\"Warning: Failed to initialize LIDA Manager: {e}. Visualization will be disabled.\")\n",
    "\n",
    "        # DataDistributor is used for database interactions\n",
    "        # DataDistributor is designed as class methods, no instance needed?\\\n",
    "        # Or initialize if it manages connections per instance?\\\n",
    "        # Based on the provided code, DataDistributor methods are @classmethod,\\\n",
    "        # so we can call them directly. No instance needed here.\\\n",
    "        # self.data_distributor = DataDistributor() # If it were an instance class\n",
    "\n",
    "    # --- New and Modified Node Methods ---\n",
    "\n",
    "    async def decide_next_step(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Decides whether to generate a visualization or continue with the text workflow.\n",
    "        Sets the visualization_request field in the state.\n",
    "        Uses the project's LLM agent for decision.\n",
    "        \"\"\"\n",
    "        question = state['question']\n",
    "        history = state.get('history', '') # Use get for safety\n",
    "\n",
    "        # Craft a prompt for the decision LLM. This might need refinement\n",
    "        # based on how your LangChainAgent handles such specific instructions.\n",
    "        decision_prompt = f\"\"\"\n",
    "Based on the following user question and chat history, determine if the user is requesting a data visualization (chart, graph, plot). Respond with a JSON object.\n",
    "\n",
    "If the user explicitly asks for a visualization, respond with:\n",
    "{{ \"request_type\": \"visualization\", \"chart_type\": \"[infer type or null]\", \"data_fields\": [\"list\", \"of\", \"relevant\", \"fields\"] }}\n",
    "\n",
    "If the user is NOT requesting a visualization, respond with:\n",
    "{{ \"request_type\": \"continue_text_workflow\" }}\n",
    "\n",
    "Your response should be a JSON object and contain ONLY the JSON object.\n",
    "\n",
    "Question: {question}\n",
    "Chat History: {history}\n",
    "\"\"\"\n",
    "        ctx.logger().debug(\"Calling LLM for decision on next step.\", eventid=\"tag_decision_llm\")\n",
    "        try:\n",
    "            # Use the QueryHandler or a dedicated LangChainAgent for this decision\n",
    "            # Assuming query_handler.ask_llm can handle this type of prompt and return structured JSON\n",
    "            # This might require specific prompt engineering or model fine-tuning.\\\n",
    "            # Alternatively, if a separate LangChainAgent is used just for decisions:\\\n",
    "            # response, metric = await self.decision_agent.process_input(user_input=decision_prompt)\n",
    "            # For now, assuming query_handler can do it, might need to adapt or create a new method.\\\n",
    "\n",
    "            # **Adaptation Required:** The structure of the original project's QueryHandler.ask_llm\\\n",
    "            # seems designed for generating chat responses, not structured decisions.\\\n",
    "            # You likely need a new method in QueryHandler (e.g., `decide_workflow`) or\\\n",
    "            # a separate LangChainAgent instance specifically configured for this task.\\\n",
    "            # For this refactor, we'll simulate the expected output based on the prompt logic.\\\n",
    "            # REPLACE THE FOLLOWING SIMULATION WITH ACTUAL LLM CALL:\n",
    "            # This is a placeholder - replace with actual LLM call using your project's classes\n",
    "            # Example using a hypothetical new method:\n",
    "            # decision_response_content, metric = await self.query_handler.decide_workflow(question, history)\n",
    "            # decision = json.loads(decision_response_content)\n",
    "\n",
    "            # --- Mock Simulation (REPLACE THIS) ---\n",
    "            # This simulates the LLM reading the prompt and deciding.\n",
    "            # In a real scenario, the LLM's response would be parsed.\n",
    "            lower_question = question.lower()\n",
    "            if any(word in lower_question for word in [\"visualization\", \"chart\", \"graph\", \"plot\"]):\n",
    "                # Attempt to infer fields based on keywords (highly simplified)\n",
    "                data_fields = []\n",
    "                if \"organization\" in lower_question: data_fields.append(\"organization\")\n",
    "                if \"category\" in lower_question: data_fields.append(\"category\")\n",
    "                if \"research_areas\" in lower_question: data_fields.append(\"research_areas\")\n",
    "                if \"keywords\" in lower_question: data_fields.append(\"keywords\")\n",
    "                if \"contacts\" in lower_question: data_fields.append(\"contacts\")\n",
    "                chart_type = \"bar\" if \"bar chart\" in lower_question else (\"line\" if \"line chart\" in lower_question else None)\n",
    "                decision = {\"request_type\": \"visualization\", \"chart_type\": chart_type, \"data_fields\": data_fields if data_fields else None}\n",
    "            else:\n",
    "                decision = {\"request_type\": \"continue_text_workflow\"}\n",
    "            # --- End Mock Simulation ---\n",
    "\n",
    "            ctx.logger().debug(f\"Decision from LLM: {decision}\", eventid=\"tag_decision_result\")\n",
    "\n",
    "            if decision.get(\"request_type\") == \"visualization\":\n",
    "                state['visualization_request'] = decision # Store the structured request\n",
    "                # Data fetching will happen in fetch_context or a dedicated node before viz generation\n",
    "            else:\n",
    "                state['visualization_request'] = None # No visualization requested\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            ctx.logger().error(\"LLM response for decision was not valid JSON.\", eventid=\"tag_decision_json_error\")\n",
    "            state['visualization_request'] = None # Default to text workflow on JSON error\n",
    "            state['visualization_error'] = \"Error parsing LLM decision.\" # Use visualization_error for consistency\n",
    "        except Exception as e:\n",
    "            ctx.logger().error(f\"Error in decide_next_step: {e}\", eventid=\"tag_decide_error\")\n",
    "            state['visualization_request'] = None # Default to text workflow on other errors\n",
    "            state['visualization_error'] = f\"Error determining visualization request: {e}\"\n",
    "\n",
    "        return state\n",
    "\n",
    "    def route_request(self, state: State) -> str:\n",
    "        \"\"\"\n",
    "        Routes the workflow based on the visualization_request state.\n",
    "        \"\"\"\n",
    "        if state.get('visualization_request'):\n",
    "            print(\"Routing to generate_visualization\")\n",
    "            return \"generate_visualization\"\n",
    "        else:\n",
    "            print(\"Routing to continue_text_workflow\")\n",
    "            return \"continue_text_workflow\"\n",
    "\n",
    "    async def fetch_context(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Fetches data based on the user's question, potentially guided by LLM tool call decision.\n",
    "        Stores raw data in raw_context_data if visualization is requested, otherwise in context string.\n",
    "        Uses the project's DataDistributor for database interaction.\n",
    "        \"\"\"\n",
    "        print(\"Executing fetch_context\")\n",
    "        question = state['question']\n",
    "        history = state.get('history', '') # Use get for safety\n",
    "        viz_request = state.get('visualization_request')\n",
    "\n",
    "        ctx.logger().debug(\"Calling LLM for tool call decision and query generation.\", eventid=\"tag_tool_llm\")\n",
    "        try:\n",
    "            # Craft a prompt for the tool calling LLM.\n",
    "            # Similar to the decision LLM, this might need a specific LangChainAgent or QueryHandler method.\n",
    "            tool_decision_prompt = f\"\"\"\n",
    "Question Instructions:\n",
    "\n",
    "Inputs:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Chat History: {history}\n",
    "\n",
    "Task: Your task is to generate an effective search query and determine the appropriate tool (vector_search_tool or parameter_search_tool) to retrieve the necessary information from the database. Consider both the question and the chat history (if relevant) when constructing your query and selecting the tool.\n",
    "If the question asks for specific attributes or uses precise terms, favor parameter_search_tool. If it's a more general or conceptual query, favor vector_search_tool.\n",
    "\n",
    "Output: Return the tool call in the specified format (e.g., using function_call in additional_kwargs).\n",
    "\n",
    "End of Instructions\n",
    "\"\"\"\n",
    "            # **Adaptation Required:** Replace with actual LLM call using your project's classes.\n",
    "            # This requires your LLM agent to support function calling or return a structure you can parse.\n",
    "            # Example using a hypothetical new method:\n",
    "            # tool_response, metric = await self.query_handler.determine_tool_call(question, history)\n",
    "            # tool_call = tool_response.additional_kwargs.get(\"function_call\", {})\n",
    "\n",
    "            # --- Mock Simulation (REPLACE THIS) ---\n",
    "            # This simulates the LLM deciding on a tool and query.\n",
    "            # In a real scenario, the LLM's response would be parsed.\n",
    "            lower_question = question.lower()\n",
    "            tool_call = {}\n",
    "            tool_query = question # Default query\n",
    "            desired_fields = [\"description\"] # Default fields for text context\n",
    "\n",
    "            if any(term in lower_question for term in [\"specific attributes\", \"precise terms\", \"details\"]):\\\n",
    "                 tool_call[\"name\"] = \"parameter_search_tool\"\n",
    "                 # Attempt to parse parameters - highly simplified mock\n",
    "                 # In reality, the LLM should provide structured arguments.\n",
    "                 parameter_query = {} # Example: {\"name\": \"Org A\"}\n",
    "                 # If visualization is requested, try to fetch specific fields identified by the decision LLM\n",
    "                 if viz_request and viz_request.get('data_fields'):\n",
    "                      desired_fields = viz_request['data_fields']\n",
    "                 else:\n",
    "                      desired_fields = [\"organization\", \"category\", \"value\"] # Example fields for parameter search data\n",
    "                 tool_query = parameter_query # Use the parameter dict as the query\n",
    "\n",
    "            else:\\\n",
    "                 tool_call[\"name\"] = \"vector_search_tool\"\n",
    "                 tool_query = question # Use the question as the vector search query\n",
    "                 # Vector search usually returns chunks/documents, format for text context initially\n",
    "                 desired_fields = [\"description\", \"file\", \"page\"] # Example fields for vector search results\n",
    "            # --- End Mock Simulation ---\n",
    "\n",
    "            func_name = tool_call.get(\"name\", \"\")\n",
    "\n",
    "            raw_output = None\n",
    "            if func_name == \"vector_search_tool\":\n",
    "                ctx.logger().debug(f\"Invoking vector search tool with query: {tool_query}\", eventid=\"tag_vector_search_invoke\")\n",
    "                # Call the actual DataDistributor method\n",
    "                # DataDistributor.vector_search expects query: str, desired_field: str\n",
    "                # The current DataDistributor.vector_search seems to return a list of strings (descriptions)\n",
    "                # If you need more structured data for viz, you might need to modify vector_search\n",
    "                # or call a different method.\n",
    "                # Assuming for now it returns list of strings or dicts depending on internal logic.\n",
    "                # Need to adapt based on what DataDistributor.vector_search actually returns.\n",
    "\n",
    "                # **Adaptation Required:** DataDistributor.vector_search currently returns a list of descriptions.\\\n",
    "                # If you need full document objects for visualization, modify DataDistributor.vector_search.\\\n",
    "                # For now, assume we call it with the question and expect a list of strings/dicts.\\\n",
    "                # We'll pass a single desired_field, assuming the method handles multiple internally if needed.\\\n",
    "                # If vector_search needs different logic for viz data, create a new method in DataDistributor.\\\n",
    "                raw_output = await DataDistributor.vector_search(query=tool_query, desired_field=\"description\")\n",
    "                ctx.logger().debug(f\"Vector search returned {len(raw_output) if raw_output else 0} results.\", eventid=\"tag_vector_search_results\")\n",
    "\n",
    "            elif func_name == \"parameter_search_tool\":\n",
    "                 ctx.logger().debug(f\"Invoking parameter search tool with query: {tool_query} and fields: {desired_fields}\", eventid=\"tag_parameter_search_invoke\")\n",
    "                 # Call the actual DataDistributor method\n",
    "                 # DataDistributor.parameter_search expects name: str, query: dict, time: bool, limit: int, des_field: list\n",
    "                 # Need collection name and query dict from the LLM's tool call arguments.\n",
    "                 # **Adaptation Required:** The mock LLM simulation doesn't provide these. The tool calling LLM needs to provide\\\n",
    "                 # collection_name and the query dict based on the user's question.\\\n",
    "                 # Assuming for the refactor we use a default collection and the LLM's 'tool_query' (which is currently just the question)\\\n",
    "                 # needs to be parsed into a query dict by the LLM or a preceding step.\\\n",
    "                 # For now, we'll use a placeholder query dict and collection name.\n",
    "\n",
    "                 # Placeholder query_dict - replace with actual parsed arguments from LLM\n",
    "                 parameter_query_dict = {\"user_query\": tool_query} # This is incorrect, needs to be a proper MongoDB query dict\n",
    "                 collection_name = Constants.config.get(\"collection1\", \"default_collection\") # Example: get default collection name\n",
    "\n",
    "                 # Need to decide on limit and time parameters - perhaps based on LLM decision or defaults?\\\n",
    "                 limit = 5 # Example default limit\n",
    "                 use_time = False # Example default\n",
    "\n",
    "                 # **Adaptation Required:** The tool_query from the LLM needs to be a proper MongoDB filter dict\\\n",
    "                 # and the collection name needs to be determined. This is a major integration point.\\\n",
    "                 # Assuming for demonstration, parameter_query_dict is derived correctly and desired_fields are populated.\\\n",
    "\n",
    "                 raw_output = await DataDistributor.parameter_search(name=collection_name, query=parameter_query_dict, time=use_time, limit=limit, des_field=desired_fields)\n",
    "                 ctx.logger().debug(f\"Parameter search returned {len(raw_output) if raw_output else 0} results.\", eventid=\"tag_parameter_search_results\")\n",
    "\n",
    "            else:\n",
    "                ctx.logger().warning(f\"Unknown tool name determined by LLM: {func_name}\", eventid=\"tag_unknown_tool\")\n",
    "                state['context'] = \"Could not identify relevant tool.\"\n",
    "                state['raw_context_data'] = None\n",
    "                state['final_answer'] = \"I couldn't find the right tool to fetch data for that.\"\n",
    "                return state # Exit early on unknown tool\n",
    "\n",
    "            if raw_output is None:\n",
    "                 ctx.logger().warning(\"Data fetching returned None.\", eventid=\"tag_fetch_none\")\n",
    "                 state['context'] = \"No data found for your query.\"\n",
    "                 state['raw_context_data'] = None\n",
    "                 # Don't set final_answer here, let subsequent nodes handle it\n",
    "\n",
    "            elif viz_request and isinstance(raw_output, list) and len(raw_output) > 0 and isinstance(raw_output[0], dict):\n",
    "                # If viz is requested and tool returned a list of dicts (structured data suitable for viz)\n",
    "                state['raw_context_data'] = raw_output # Store raw data list of dicts\n",
    "                state['context'] = \"Data fetched for visualization.\"\n",
    "                state['summarized_context'] = \"\" # Clear summarized context for viz path\n",
    "                ctx.logger().debug(f\"Stored {len(raw_output)} items in raw_context_data for visualization.\", eventid=\"tag_stored_raw_viz\")\n",
    "            else:\n",
    "                # If no viz request, or data is not list of dicts (e.g., vector search returns list of strings)\n",
    "                # Convert raw output to a string for text workflow summarization\n",
    "                # Ensure output is iterable for join\n",
    "                if isinstance(raw_output, list):\n",
    "                    context_string = \"\\n\".join(map(str, raw_output)) # Join items into a string\n",
    "                else:\\\n",
    "                     context_string = str(raw_output) # Convert single item or other types to string\n",
    "\n",
    "                state['context'] = context_string\n",
    "                state['raw_context_data'] = None # Ensure raw data is clear if not used for viz\n",
    "                state['summarized_context'] = \"\" # Summarization will happen in the next node\n",
    "                ctx.logger().debug(\"Formatted fetched data into context string for text workflow.\", eventid=\"tag_formatted_context\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            ctx.logger().error(f\"Error in fetch_context: {e}\", eventid=\"tag_fetch_error\")\n",
    "            state['context'] = f\"Error fetching data: {e}\"\n",
    "            state['raw_context_data'] = None\n",
    "            state['summarized_context'] = \"\"\n",
    "            state['final_answer'] = \"An error occurred while fetching data.\"\n",
    "            state['visualization_error'] = f\"Data fetching error: {e}\" # Use viz error field for consistency\n",
    "            # Optionally, re-raise the exception if you want the graph to stop\n",
    "            # raise e\n",
    "\n",
    "        return state\n",
    "\n",
    "    async def summarize_context(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Summarizes the fetched context for the text workflow.\n",
    "        This node is skipped if a visualization is requested or if there's no context.\n",
    "        Uses the project's LLM agent for summarization.\n",
    "        \"\"\"\n",
    "        print(\"Executing summarize_context\")\n",
    "        context = state.get('context')\n",
    "\n",
    "        # Skip summarization if visualization is requested or no context to summarize\n",
    "        if state.get('visualization_request') or not context or context == \"Data fetched for visualization.\":\\\n",
    "            print(\"Skipping summarization as visualization is requested or no context.\")\n",
    "            state['summarized_context'] = \"\" # Ensure empty if skipped\n",
    "            return state\n",
    "\n",
    "        ctx.logger().debug(\"Calling LLM for context summarization.\", eventid=\"tag_summarize_llm\")\n",
    "        summary_prompt = f\"\"\"Summarize the following context for a chatbot response:\n",
    "{context}\n",
    "\"\"\"\n",
    "        try:\n",
    "            # Use the QueryHandler.summarize_llm method\n",
    "            summary_response_content, metric = await self.query_handler.summarize_llm(question=summary_prompt)\n",
    "            state['summarized_context'] = summary_response_content\n",
    "            ctx.logger().debug(\"Context summarization complete.\", eventid=\"tag_summarize_complete\")\n",
    "        except Exception as e:\n",
    "            ctx.logger().error(f\"Error in summarize_context: {e}\", eventid=\"tag_summarize_error\")\n",
    "            state['summarized_context'] = \"Error summarizing context.\"\n",
    "            state['final_answer'] = \"An error occurred while summarizing the information.\"\n",
    "            # Optionally, re-raise the exception\n",
    "            # raise e\n",
    "\n",
    "        return state\n",
    "\n",
    "    async def generate_answer(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Generates the final text answer based on the summarized context.\n",
    "        This node is skipped if a visualization was successfully generated.\n",
    "        Uses the project's LLM agent for answer generation.\n",
    "        \"\"\"\n",
    "        print(\"Executing generate_answer\")\n",
    "        question = state['question']\n",
    "        summarized_context = state.get('summarized_context')\n",
    "        history = state.get('history', '') # Use get for safety\n",
    "        visualization_output = state.get('visualization_output')\n",
    "\n",
    "        # Skip text answer generation if visualization output exists and is successful\n",
    "        if visualization_output:\n",
    "             print(\"Skipping generate_answer as visualization output exists.\")\n",
    "             # The final_answer might have been set by generate_visualization_node\n",
    "             return state\n",
    "\n",
    "        if not summarized_context:\n",
    "            # If no summarized context and no viz output, set a default error message\n",
    "            if not state.get('final_answer'): # Avoid overwriting a previous error message\n",
    "                 state['final_answer'] = state.get('context', \"Could not generate a response based on the available information.\")\n",
    "                 if state['final_answer'] == \"Data fetched for visualization.\":\n",
    "                      state['final_answer'] = \"Could not generate a response based on the available information.\"\n",
    "            print(\"Skipping generate_answer due to missing summarized context.\")\n",
    "            return state\n",
    "\n",
    "        ctx.logger().debug(\"Calling LLM for final answer generation.\", eventid=\"tag_answer_llm\")\n",
    "        answer_prompt = f\"\"\"Based on the following summarized context and chat history, answer the user's question:\n",
    "\n",
    "Question: {question}\n",
    "Chat History: {history}\n",
    "Summarized Context: {summarized_context}\n",
    "\n",
    "Provide a concise and helpful answer.\n",
    "\"\"\"\n",
    "        try:\n",
    "            # Use the QueryHandler.ask_llm method for the final answer\n",
    "            # QueryHandler.ask_llm expects question and history\n",
    "            # We'll pass the full prompt including summarized context as the question for now.\n",
    "            # **Adaptation Required:** QueryHandler.ask_llm is designed for initial query processing.\\\n",
    "            # You might need a new method in QueryHandler (e.g., `generate_final_response`) that takes\\\n",
    "            # summarized context explicitly, or adapt how ask_llm uses the prompt/history.\\\n",
    "\n",
    "            # For this refactor, we'll simulate using ask_llm with the combined prompt.\\\n",
    "            # REPLACE THE FOLLOWING SIMULATION WITH ACTUAL LLM CALL:\n",
    "            # answer_response_content, metric = await self.query_handler.ask_llm(question=answer_prompt, history=history)\n",
    "            # state['final_answer'] = answer_response_content\n",
    "\n",
    "            # --- Mock Simulation (REPLACE THIS) ---\n",
    "            state['final_answer'] = f\"Answer based on: {summarized_context[:100]}... (Actual answer generation via LLM)\"\n",
    "            # --- End Mock Simulation ---\n",
    "\n",
    "            ctx.logger().debug(\"Final answer generation complete.\\\n",
    "\n",
    "\", eventid=\"tag_answer_complete\")\n",
    "\n",
    "        except Exception as e:\n",
    "            ctx.logger().error(f\"Error in generate_answer: {e}\", eventid=\"tag_answer_error\")\n",
    "            state['final_answer'] = \"An error occurred while generating the answer.\"\n",
    "            # Optionally, re-raise the exception\n",
    "            # raise e\n",
    "\n",
    "        return state\n",
    "\n",
    "    async def generate_visualization_node(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Calls the LIDA tool logic to generate a visualization.\n",
    "        Requires raw_context_data to be populated with a list of dictionaries.\n",
    "        \"\"\"\n",
    "        print(\"Executing generate_visualization_node\")\n",
    "        question = state['question']\n",
    "        raw_data = state.get('raw_context_data') # Use raw_context_data\n",
    "        viz_request = state.get('visualization_request')\n",
    "\n",
    "        if not self.lida:\n",
    "             error_msg = \"LIDA Manager was not initialized successfully. Visualization is disabled.\"\n",
    "             ctx.logger().error(error_msg, eventid=\"tag_viz_lida_not_initialized\")\n",
    "             state['final_answer'] = error_msg\n",
    "             state['visualization_error'] = error_msg\n",
    "             return state\n",
    "\n",
    "        if not raw_data or not isinstance(raw_data, list) or len(raw_data) == 0 or not isinstance(raw_data[0], dict):\n",
    "            error_msg = \"I couldn't find enough structured data to create a visualization for that.\"\n",
    "            ctx.logger().warning(error_msg, eventid=\"tag_viz_no_structured_data\")\n",
    "            state['final_answer'] = error_msg\n",
    "            state['visualization_error'] = \"No structured data available for visualization.\"\n",
    "            return state\n",
    "\n",
    "        try:\n",
    "            # Transform data for LIDA (list of dicts to DataFrame)\n",
    "            df = data_to_dataframe(raw_data)\n",
    "            if df.empty:\n",
    "                 error_msg = \"Failed to convert fetched data to DataFrame.\"\n",
    "                 ctx.logger().error(error_msg, eventid=\"tag_viz_dataframe_error\")\n",
    "                 state['final_answer'] = error_msg\n",
    "                 state['visualization_error'] = \"DataFrame conversion failed.\"\n",
    "                 return state\n",
    "\n",
    "            ctx.logger().debug(\"Generating data summary using LIDA.\", eventid=\"tag_lida_summarize\")\n",
    "            # Summarize the data using LIDA\n",
    "            data_summary = self.lida.summarize(data_frame=df, data_config=DataConfig())\n",
    "            ctx.logger().debug(f\"LIDA Data Summary generated.\", eventid=\"tag_lida_summary_complete\")\n",
    "\n",
    "            # Use the user's query or the parsed viz_request for visualization generation\n",
    "            # Prioritize the original question as the goal for LIDA\n",
    "            visualization_goal = question\n",
    "\n",
    "            ctx.logger().debug(\"Generating visualization code specs using LIDA.\", eventid=\"tag_lida_generate_viz\")\n",
    "            # Generate visualization code specs\n",
    "            # Use the same text gen config as LIDA manager init or a new one if needed\n",
    "            viz_code_specs = self.lida.generate_viz(\n",
    "                summary=data_summary,\n",
    "                goal=visualization_goal,\n",
    "                textgen_config=self.lida.text_gen_config, # Use the manager's config\n",
    "                library=\"altair\" # Specify a library LIDA should use (altair is common for vega-lite)\n",
    "            )\n",
    "            ctx.logger().debug(f\"LIDA Generated {len(viz_code_specs)} Viz Code Specs.\", eventid=\"tag_lida_viz_specs_complete\")\n",
    "\n",
    "            if viz_code_specs:\n",
    "                # Execute the visualization code - try the first spec\n",
    "                ctx.logger().debug(\"Executing visualization code using LIDA.\", eventid=\"tag_lida_execute_viz\")\n",
    "                charts = self.lida.execute_viz(\n",
    "                    code_specs=viz_code_specs[0].code, # Assuming the first spec is relevant and has a 'code' field\n",
    "                    data=df,\n",
    "                    summary=data_summary,\n",
    "                    library=\"altair\"\n",
    "                )\n",
    "                ctx.logger().debug(f\"LIDA Executed {len(charts) if charts else 0} charts.\", eventid=\"tag_lida_execute_complete\")\n",
    "\n",
    "                if charts:\n",
    "                    # Assuming charts is a list of dictionaries, each with 'spec' (Vega-Lite JSON) and potentially 'raster' (image)\n",
    "                    first_chart = charts[0]\n",
    "                    if 'spec' in first_chart and first_chart['spec'] is not None:\n",
    "                        state['visualization_output'] = {\n",
    "                            \"type\": \"vega-lite\",\n",
    "                            \"spec\": first_chart['spec'] # Vega-Lite JSON specification\n",
    "                        }\n",
    "                        # Set a default success message, can be refined later if needed\n",
    "                        state['final_answer'] = \"Here is the visualization you requested:\"\n",
    "                        ctx.logger().info(\"Successfully generated Vega-Lite visualization.\", eventid=\"tag_viz_success_vega\")\n",
    "                    elif 'raster' in first_chart and first_chart['raster'] is not None:\n",
    "                        # If no spec but image is available, return the image (base64)\n",
    "                        # LIDA might return base64 directly or a path. Check LIDA docs/output.\n",
    "                        # Assume it's base64 for now.\n",
    "                        state['visualization_output'] = {\n",
    "                            \"type\": \"image\",\n",
    "                            \"data\": first_chart['raster'] # Base64 encoded image data\n",
    "                        }\n",
    "                        state['final_answer'] = \"Here is the visualization you requested:\"\n",
    "                        ctx.logger().info(\"Successfully generated visualization image.\", eventid=\"tag_viz_success_image\")\n",
    "                    else:\n",
    "                        error_msg = \"I generated a visualization, but I couldn't format it correctly.\"\n",
    "                        ctx.logger().error(error_msg, eventid=\"tag_viz_format_error\")\n",
    "                        state['final_answer'] = error_msg\n",
    "                        state['visualization_error'] = \"LIDA execution returned unexpected format.\"\n",
    "\n",
    "                else:\n",
    "                    error_msg = \"I was able to generate visualization code, but executing it failed.\"\n",
    "                    ctx.logger().error(error_msg, eventid=\"tag_viz_execute_failed\")\n",
    "                    state['final_answer'] = error_msg\n",
    "                    state['visualization_error'] = \"LIDA execute_viz failed.\"\n",
    "\n",
    "            else:\n",
    "                error_msg = \"I understood you wanted a visualization, but I couldn't generate a suitable one based on the data and your request.\"\n",
    "                ctx.logger().warning(error_msg, eventid=\"tag_viz_generate_failed\")\n",
    "                state['final_answer'] = error_msg\n",
    "                state['visualization_error'] = \"LIDA generate_viz failed.\"\n",
    "\n",
    "        except ImportError as e:\n",
    "            error_msg = f\"Missing required library for visualization: {e}. Please install it (e.g., pip install lida pandas altair).\"\\\n",
    "            ctx.logger().error(error_msg, eventid=\"tag_viz_import_missing\")\n",
    "            state['final_answer'] = error_msg\n",
    "            state['visualization_error'] = f\"Import error: {e}\"\n",
    "        except Exception as e:\n",
    "            ctx.logger().error(f\"Error during LIDA visualization generation: {e}\", eventid=\"tag_viz_general_error\")\n",
    "            state['final_answer'] = \"An error occurred while trying to generate the visualization.\"\n",
    "            state['visualization_error'] = f\"LIDA generation error: {e}\"\n",
    "\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151f13b",
   "metadata": {},
   "source": [
    "## Workflow Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workflow(self):\n",
    "        \"\"\"\n",
    "        Defines the LangGraph workflow including visualization path.\n",
    "        \"\"\"\n",
    "        workflow = StateGraph(State)\n",
    "\n",
    "        # Add nodes\n",
    "        workflow.add_node(\"decide_next_step\", self.decide_next_step)\n",
    "        workflow.add_node(\"fetch_context\", self.fetch_context)\n",
    "        workflow.add_node(\"summarize\", self.summarize_context)\n",
    "        workflow.add_node(\"generate_answer\", self.generate_answer)\n",
    "        workflow.add_node(\"generate_visualization\", self.generate_visualization_node)\n",
    "\n",
    "        # Define edges\n",
    "        workflow.add_edge(START, \"decide_next_step\") # Start by deciding text vs viz\n",
    "\n",
    "        # After deciding, fetch context regardless of path, as both need data\n",
    "        # The fetch_context node will handle storing raw data for viz or string for text\n",
    "        workflow.add_edge(\"decide_next_step\", \"fetch_context\")\n",
    "\n",
    "        # After fetching data, route based on the decision made in decide_next_step\n",
    "        workflow.add_conditional_edge(\n",
    "            \"fetch_context\", # Route from fetch_context now\n",
    "            self.route_request, # Method to determine the next node\n",
    "            {\n",
    "                \"generate_visualization\": \"generate_visualization\", # If visualization is requested\n",
    "                \"continue_text_workflow\": \"summarize\" # If a standard text answer is needed\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Edges from visualization and text workflows\n",
    "        # Visualization path goes directly to END after generation attempt\n",
    "        workflow.add_edge(\"generate_visualization\", END)\n",
    "\n",
    "        # Text workflow continues from summarize to generate_answer\n",
    "        workflow.add_edge(\"summarize\", \"generate_answer\")\n",
    "\n",
    "        # Text answer path goes to END\n",
    "        workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "        return workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b581231",
   "metadata": {},
   "source": [
    "## Usage in Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to use in your Jupyter Notebook ---\n",
    "\n",
    "# 1. Initialize the GraphBuilder\n",
    "#    This will initialize LLM agents, DataDistributor (conceptually), and LIDA.\n",
    "#    Ensure your environment variables and Constants are set up correctly.\n",
    "graph_builder = GraphBuilder()\n",
    "\n",
    "# 2. Create the workflow\n",
    "workflow = graph_builder.create_workflow()\n",
    "\n",
    "# 3. Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# 4. Run the graph with an initial state\n",
    "# You'll need to provide the initial state dictionary.\n",
    "# The 'question' and 'history' are the primary inputs.\n",
    "# Other fields are populated during the workflow.\n",
    "\n",
    "# Example state for a text query:\n",
    "initial_state_text = {\n",
    " 'messages': [], # Assuming messages are tracked here\n",
    " 'history': 'User: What is the purpose of the project? AI: The project is a chatbot.', # Example history format\n",
    " 'question': 'Tell me about the organizations mentioned in the data.',\n",
    " # Other state fields will be populated by the graph nodes\n",
    " 'should_split': False,\n",
    " 'split_questions': [],\n",
    " 'needs_history': False,\n",
    " 'needs_context': False,\n",
    " 'context': '',\n",
    " 'raw_context_data': None,\n",
    " 'summarized_context': '',\n",
    " 'final_answer': '',\n",
    " 'visualization_request': None,\n",
    " 'visualization_output': None,\n",
    " 'visualization_error': None\n",
    "}\n",
    "\n",
    "# Example state for a visualization query:\n",
    "initial_state_viz = {\n",
    " 'messages': [],\n",
    " 'history': '',\n",
    " 'question': 'Show me a bar chart of organizations by category.',\n",
    " # Other state fields will be populated by the graph nodes\n",
    " 'should_split': False,\n",
    " 'split_questions': [],\n",
    " 'needs_history': False,\n",
    " 'needs_context': False,\n",
    " 'context': '',\n",
    " 'raw_context_data': None,\n",
    " 'summarized_context': '',\n",
    " 'final_answer': '',\n",
    " 'visualization_request': None, # This will be set by decide_next_step\n",
    " 'visualization_output': None, # This will be populated by generate_visualization\n",
    " 'visualization_error': None\n",
    "}\n",
    "\n",
    "# To run the graph (use await in an async environment like certain parts of Jupyter or inside an async function)\n",
    "# For a simple test in a synchronous notebook cell, you need to run it within an asyncio event loop.\n",
    "\n",
    "async def run_graph_async(state):\n",
    "    # Wrap the graph execution in a RequestContext for logging\n",
    "    # You need to provide actual user_id, correlation_id, session_id, client_id, body\n",
    "    # based on how your project generates these.\n",
    "    # For notebook testing, you can use placeholders.\n",
    "    with ctx(\n",
    "        user_id=\"test_user\",\n",
    "        correlation_id=\"test_corr\",\n",
    "        session_id=\"test_session\",\n",
    "        client_id=\"test_client\",\n",
    "        body={\"question\": state['question'], \"history\": state['history']}\n",
    "    ):\n",
    "        print(\"Running graph...\")\n",
    "        # The graph runs until it reaches END\n",
    "        result = await app.ainvoke(state)\n",
    "        print(\"Graph execution finished.\")\n",
    "        return result\n",
    "\n",
    "# Example usage (run one at a time by uncommenting):\n",
    "\n",
    "# print(\"Running text query...\")\n",
    "# text_result = await run_graph_async(initial_state_text)\n",
    "# print(\"\\n--- Text Workflow Result ---\")\n",
    "# print(f\"Final Answer: {text_result.get('final_answer')}\")\n",
    "# print(f\"Context: {text_result.get('context')[:200]}...\")\n",
    "# print(f\"Summarized Context: {text_result.get('summarized_context')[:200]}...\")\n",
    "# print(f\"Visualization Request: {text_result.get('visualization_request')}\")\n",
    "# print(f\"Visualization Output: {text_result.get('visualization_output')}\")\n",
    "# print(f\"Visualization Error: {text_result.get('visualization_error')}\")\n",
    "\n",
    "\n",
    "print(\"Running visualization query...\")\n",
    "# For visualization, ensure your MongoDB has data that DataDistributor can fetch\n",
    "# and that LIDA can process (list of dicts format is crucial).\n",
    "# You might need to set up a mock MongoDB or ensure connection details are correct.\n",
    "viz_result = await run_graph_async(initial_state_viz)\n",
    "\n",
    "print(\"\\n--- Visualization Workflow Result ---\")\n",
    "print(f\"Final Answer (Text Intro): {viz_result.get('final_answer')}\")\n",
    "# The actual visualization output will be in viz_result.get('visualization_output')\n",
    "# This will likely be a dictionary containing the Vega-Lite spec or image data.\n",
    "# You'll need to render this output in your frontend/UI.\n",
    "viz_output = viz_result.get('visualization_output')\n",
    "if viz_output:\n",
    "    print(f\"Visualization Output Type: {viz_output.get('type')}\")\n",
    "    if viz_output.get('type') == 'vega-lite':\n",
    "        print(\"Vega-Lite Spec (truncated):\", json.dumps(viz_output.get('spec'), indent=2)[:500], \"...\")\n",
    "        # In a real notebook, you might display this using altair or ipyvega\n",
    "        # import altair as alt\n",
    "        # alt.Chart.from_dict(viz_output['spec']).display()\n",
    "    elif viz_output.get('type') == 'image':\n",
    "         print(\"Image Data (truncated):\", viz_output.get('data', '')[:100], \"...\")\n",
    "         # In a real notebook, you might display this using IPython.display.Image\n",
    "         # from IPython.display import Image\n",
    "         # Image(base64.b64decode(viz_output['data'])) # Requires base64 import\n",
    "else:\n",
    "    print(\"Visualization Output: None\")\n",
    "\n",
    "print(f\"Visualization Request: {viz_result.get('visualization_request')}\")\n",
    "print(f\"Raw Context Data (truncated): {str(viz_result.get('raw_context_data', ''))[:200]}...\\\n",
    "\n",
    "\")\n",
    "print(f\"Visualization Error: {viz_result.get('visualization_error')}\")\n",
    "print(f\"Context (for text path, likely empty here): {viz_result.get('context')}\")\n",
    "print(f\"Summarized Context (for text path, likely empty here): {viz_result.get('summarized_context')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36167069",
   "metadata": {},
   "source": [
    "\n",
    "##### Notes for reintegration:\n",
    "##### 1. This notebook provides the core LangGraph workflow logic.\n",
    "##### 2. Integrate the `GraphBuilder` class and its methods into your project's structure (e.g., a new module or within an existing one).\n",
    "##### 3. Ensure all necessary project imports (`DataDistributor`, `Constants`, `RequestContext`, `Chain`, etc.) are correctly resolved in the final location.\n",
    "##### 4. The `RequestContext` usage in `run_graph_async` demonstrates how to wrap the graph execution to provide context for logging within the graph nodes. Adapt this to how your server/entry point handles incoming requests.\n",
    "##### 5. The handling of `visualization_output` needs to be integrated into your frontend or API response structure to render the chart or image.\n",
    "##### 6. Refine the LLM prompts and the parsing of LLM responses (especially for tool calling and decision making) based on the capabilities and expected output format of your actual LangChainAgent instances or QueryHandler methods.\n",
    "##### 7. Ensure robust error handling is propagated correctly through the graph and reported using your project's logging.\n",
    "##### 8. Review the data fetching logic in `Workspace_context` to ensure it correctly calls `DataDistributor.vector_search` or `DataDistributor.parameter_search` with the right arguments derived from the LLM's tool call decision.\n",
    "##### 9. Verify that `data_to_dataframe` correctly handles the format of data returned by your DataDistributor methods when used for visualization.\n",
    "##### 10. Confirm LIDA's compatibility with your chosen LLM model and ensure the API key is securely managed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternotebooks-ej2AvU7G-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
