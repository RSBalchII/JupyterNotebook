{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a890d4",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c563f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Annotated, TypedDict, Optional\n",
    "\n",
    "# Assuming these are available or you will implement them\n",
    "# from DataDistributor import DataDistributor\n",
    "# from Constants import Constants # For API keys etc.\n",
    "\n",
    "# You will need to replace this with your actual LLM initialization\n",
    "# from langchain_core.language_models import BaseChatModel\n",
    "# from langchain_core.output_parsers import JsonOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- LangGraph Setup ---\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "# from langgraph.prebuilt import ToolExecutor # You will likely use this in your real implementation\n",
    "from langchain.tools import BaseTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea949bc9",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35092db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for your tool implementations\n",
    "# You will need to replace this with your actual tool definitions\n",
    "# from your existing project (e.g., vector_search, parameter_search)\n",
    "class MockVectorSearchTool:\n",
    "    def invoke(self, query):\n",
    "        print(f\"MockVectorSearchTool invoked with query: {query}\")\n",
    "        # Return mock structured data for visualization testing\n",
    "        return [\n",
    "            {\"organization\": \"Org A\", \"category\": \"Category 1\", \"value\": 100},\n",
    "            {\"organization\": \"Org B\", \"category\": \"Category 2\", \"value\": 150},\n",
    "            {\"organization\": \"Org A\", \"category\": \"Category 1\", \"value\": 120},\n",
    "            {\"organization\": \"Org C\", \"category\": \"Category 1\", \"value\": 80},\n",
    "        ]\n",
    "\n",
    "class MockParameterSearchTool:\n",
    "    def invoke(self, query):\n",
    "        print(f\"MockParameterSearchTool invoked with query: {query}\")\n",
    "        # Return mock structured data for visualization testing\n",
    "        return [\n",
    "            {\"organization\": \"Org A\", \"category\": \"Category 1\", \"value\": 100},\n",
    "            {\"organization\": \"Org B\", \"category\": \"Category 2\", \"value\": 150},\n",
    "            {\"organization\": \"Org A\", \"category\": \"Category 1\", \"value\": 120},\n",
    "            {\"organization\": \"Org C\", \"category\": \"Category 1\", \"value\": 80},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8789eb",
   "metadata": {},
   "source": [
    "## DataFraming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579077b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You will need to implement this based on your data structure\n",
    "def data_to_dataframe(raw_data: list) -> 'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Transforms raw data (list of dicts) into a Pandas DataFrame.\n",
    "    You will need pandas installed: pip install pandas\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    print(\"Transforming data to DataFrame...\")\n",
    "    # Add logic to flatten or process data if needed\n",
    "    return pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49164d76",
   "metadata": {},
   "source": [
    "## LLM Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock LLM classes for decision making and response generation\n",
    "class MockDecisionLLM:\n",
    "    async def ainvoke(self, prompt: str):\n",
    "        print(f\"MockDecisionLLM received prompt:\\n{prompt}\")\n",
    "        # Simple mock logic to decide based on keywords\n",
    "        if \"visualization\" in prompt.lower() or \"chart\" in prompt.lower() or \"graph\" in prompt.lower() or \"plot\" in prompt.lower():\n",
    "             # Simulate a visualization request decision\n",
    "             return HumanMessage(content='{{\"request_type\": \"visualization\", \"chart_type\": \"bar\", \"data_fields\": [\"organization\", \"category\"]}}')\n",
    "        else:\n",
    "             # Simulate a text workflow decision\n",
    "             return HumanMessage(content='{{\"request_type\": \"continue_text_workflow\"}}')\n",
    "\n",
    "class MockToolCallingLLM:\n",
    "     async def ainvoke(self, prompt: str):\n",
    "        print(f\"MockToolCallingLLM received prompt:\\n{prompt}\")\n",
    "        # Simple mock logic to decide which tool to call\n",
    "        if \"specific attributes\" in prompt.lower() or \"precise terms\" in prompt.lower():\n",
    "            tool_name = \"parameter_search_tool\"\n",
    "        else:\n",
    "            tool_name = \"vector_search_tool\"\n",
    "        # Simulate a tool call\n",
    "        return HumanMessage(content=json.dumps({\n",
    "            \"additional_kwargs\": {\n",
    "                \"function_call\": {\n",
    "                    \"name\": tool_name,\n",
    "                    \"arguments\": \"{}\" # Mock arguments, refine as needed\n",
    "                }\n",
    "            }\n",
    "        }))\n",
    "\n",
    "\n",
    "class MockSummarizeLLM:\n",
    "     async def ainvoke(self, prompt: str):\n",
    "        print(f\"MockSummarizeLLM received prompt:\\n{prompt}\")\n",
    "        # Simulate a summary\n",
    "        return HumanMessage(content=\"This is a summarized context based on the search results.\")\n",
    "\n",
    "class MockGenerateAnswerLLM:\n",
    "     async def ainvoke(self, prompt: str):\n",
    "        print(f\"MockGenerateAnswerLLM received prompt:\\n{prompt}\")\n",
    "        # Simulate generating a text answer\n",
    "        return HumanMessage(content=\"This is the final text answer based on the summary.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048da3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "640f24f8",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975dcedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for your add_messages function\n",
    "def add_messages(left: list, right: list):\n",
    "    \"\"\"Append the new messages to the existing messages.\"\"\"\n",
    "    left.extend(right)\n",
    "    return left\n",
    "\n",
    "# Update State TypedDict\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    history: str\n",
    "    question: str\n",
    "    should_split: bool\n",
    "    split_questions: list\n",
    "    needs_history: bool\n",
    "    needs_context: bool\n",
    "    context: str\n",
    "    summarized_context: str\n",
    "    final_answer: str\n",
    "\n",
    "    # New fields for visualization\n",
    "    visualization_request: Optional[dict]\n",
    "    visualization_data: Optional[list]\n",
    "    visualization_output: Optional[dict]\n",
    "    visualization_error: Optional[str]\n",
    "\n",
    "# You will wrap your logic in a class like this\n",
    "class GraphBuilder:\n",
    "    def __init__(self):\n",
    "        # Initialize your actual LLM models and tools here\n",
    "        self.decision_model = MockDecisionLLM() # Replace with your LLM for decisions\n",
    "        self.tool_model = MockToolCallingLLM() # Replace with your LLM for tool calling\n",
    "        self.summarize_model = MockSummarizeLLM() # Replace with your LLM for summarization\n",
    "        self.answer_model = MockGenerateAnswerLLM() # Replace with your LLM for answer generation\n",
    "\n",
    "        # Initialize your actual tools here\n",
    "        self.tools = {\n",
    "            \"vector_search_tool\": MockVectorSearchTool(), # Replace with your vector search tool\n",
    "            \"parameter_search_tool\": MockParameterSearchTool(), # Replace with your parameter search tool\n",
    "        }\n",
    "        # self.tool_executor = ToolExecutor(list(self.tools.values())) # Use ToolExecutor in real implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151f13b",
   "metadata": {},
   "source": [
    "## LIDA Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Placeholder for LIDA Manager - requires installation: pip install lida\n",
    "        # And you'll need an API key for LIDA's text generation model\n",
    "        # from lida import Manager, TextGenerationConfig, DataConfig\n",
    "        # from Constants import Constants # Assuming Constants holds your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b581231",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_workflow(self):\n",
    "        workflow = StateGraph(State)\n",
    "\n",
    "        # Existing nodes (placeholders for your actual implementations)\n",
    "        workflow.add_node(\"fetch_context\", self.fetch_context) # Fetches data based on query\n",
    "        workflow.add_node(\"summarize\", self.summarize_context) # Summarizes context for text answer\n",
    "        workflow.add_node(\"generate_answer\", self.generate_answer) # Generates final text answer\n",
    "\n",
    "        # New node for visualization\n",
    "        workflow.add_node(\"generate_visualization\", self.generate_visualization_node) # Will call the LIDA tool logic\n",
    "\n",
    "        # Add a decision node\n",
    "        workflow.add_node(\"decide_next_step\", self.decide_next_step)\n",
    "\n",
    "        # Define edges\n",
    "        workflow.add_edge(START, \"fetch_context\")\n",
    "        workflow.add_edge(\"fetch_context\", \"decide_next_step\") # After fetching data, decide what to do next\n",
    "\n",
    "        # Conditional edge from decide_next_step\n",
    "        workflow.add_conditional_edge(\n",
    "            \"decide_next_step\",\n",
    "            self.route_request, # Method to determine the next node\n",
    "            {\n",
    "                \"generate_visualization\": \"generate_visualization\", # If visualization is requested\n",
    "                \"continue_text_workflow\": \"summarize\" # If a standard text answer is needed\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Edges from new nodes\n",
    "        # Note: In your plan, generate_visualization goes to END.\n",
    "        # If you need to potentially add a text response *after* visualization,\n",
    "        # you might route it to 'generate_answer' instead of END, and modify generate_answer\n",
    "        # to handle both text-only and viz+text responses. For this setup,\n",
    "        # we'll follow your plan and route viz to END.\n",
    "        workflow.add_edge(\"generate_visualization\", END)\n",
    "        workflow.add_edge(\"summarize\", \"generate_answer\") # Existing text workflow continues\n",
    "        workflow.add_edge(\"generate_answer\", END) # End of text workflow\n",
    "\n",
    "        return workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4433d",
   "metadata": {},
   "source": [
    "Node Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af234f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_workflow(self):\n",
    "        workflow = StateGraph(State)\n",
    "\n",
    "        # Existing nodes (placeholders for your actual implementations)\n",
    "        workflow.add_node(\"fetch_context\", self.fetch_context) # Fetches data based on query\n",
    "        workflow.add_node(\"summarize\", self.summarize_context) # Summarizes context for text answer\n",
    "        workflow.add_node(\"generate_answer\", self.generate_answer) # Generates final text answer\n",
    "\n",
    "        # New node for visualization\n",
    "        workflow.add_node(\"generate_visualization\", self.generate_visualization_node) # Will call the LIDA tool logic\n",
    "\n",
    "        # Add a decision node\n",
    "        workflow.add_node(\"decide_next_step\", self.decide_next_step)\n",
    "\n",
    "        # Define edges\n",
    "        workflow.add_edge(START, \"fetch_context\")\n",
    "        workflow.add_edge(\"fetch_context\", \"decide_next_step\") # After fetching data, decide what to do next\n",
    "\n",
    "        # Conditional edge from decide_next_step\n",
    "        workflow.add_conditional_edge(\n",
    "            \"decide_next_step\",\n",
    "            self.route_request, # Method to determine the next node\n",
    "            {\n",
    "                \"generate_visualization\": \"generate_visualization\", # If visualization is requested\n",
    "                \"continue_text_workflow\": \"summarize\" # If a standard text answer is needed\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Edges from new nodes\n",
    "        # Note: In your plan, generate_visualization goes to END.\n",
    "        # If you need to potentially add a text response *after* visualization,\n",
    "        # you might route it to 'generate_answer' instead of END, and modify generate_answer\n",
    "        # to handle both text-only and viz+text responses. For this setup,\n",
    "        # we'll follow your plan and route viz to END.\n",
    "        workflow.add_edge(\"generate_visualization\", END)\n",
    "        workflow.add_edge(\"summarize\", \"generate_answer\") # Existing text workflow continues\n",
    "        workflow.add_edge(\"generate_answer\", END) # End of text workflow\n",
    "\n",
    "        return workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44777e0",
   "metadata": {},
   "source": [
    "##Node Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- New and Modified Node Methods ---\n",
    "\n",
    "    async def decide_next_step(self, state: State) -> State:\n",
    "         \"\"\"\n",
    "         Decides whether to generate a visualization or continue with the text workflow.\n",
    "         If visualization is requested, attempts to fetch the specific data needed.\n",
    "         Sets the visualization_request and visualization_data fields in the state.\n",
    "         \"\"\"\n",
    "         question = state['question']\n",
    "         history = state.get('history', '')\n",
    "         decision_prompt = f\"\"\"\n",
    "         Based on the following user question and chat history, determine if the user is requesting a data visualization (chart, graph, plot).\n",
    "\n",
    "         If the user explicitly asks for a visualization (e.g., \"show me a bar chart\", \"plot X vs Y\", \"graph the distribution of Z\"), respond with a JSON object indicating:\n",
    "         1. \"request_type\": \"visualization\"\n",
    "         2. \"chart_type\": Infer the type if mentioned (e.g., \"bar\", \"line\", \"scatter\"), otherwise null.\n",
    "         3. \"data_fields\": A list of the specific fields from the data (\"organization\", \"category\", \"research_areas\", \"keywords\", \"contacts\", etc.) that are needed to create this visualization. Analyze the question carefully to identify the relevant fields.\n",
    "\n",
    "         If the user is NOT requesting a visualization, respond with a JSON object indicating:\n",
    "         1. \"request_type\": \"continue_text_workflow\"\n",
    "\n",
    "         Your response should be a JSON object and contain ONLY the JSON object.\n",
    "         \"\"\"\n",
    "         try:\n",
    "             response = await self.decision_model.ainvoke(decision_prompt)\n",
    "             decision = json.loads(response.content.strip())\n",
    "             print(f\"Decision from LLM: {decision}\")\n",
    "\n",
    "             if decision.get(\"request_type\") == \"visualization\":\n",
    "                  state['visualization_request'] = decision # Store the structured request\n",
    "                  # Data fetching will happen in fetch_context or a dedicated node before viz generation\n",
    "             else:\n",
    "                 state['visualization_request'] = None # No visualization requested\n",
    "\n",
    "         except Exception as e:\n",
    "             print(f\"Error in decide_next_step: {e}\")\n",
    "             state['visualization_request'] = None\n",
    "             state['visualization_error'] = f\"Error determining visualization request: {e}\"\n",
    "\n",
    "         return state\n",
    "\n",
    "    def route_request(self, state: State) -> str:\n",
    "        \"\"\"\n",
    "        Routes the workflow based on the visualization_request state.\n",
    "        \"\"\"\n",
    "        if state.get('visualization_request'):\n",
    "            print(\"Routing to generate_visualization\")\n",
    "            return \"generate_visualization\"\n",
    "        else:\n",
    "            print(\"Routing to continue_text_workflow\")\n",
    "            return \"continue_text_workflow\"\n",
    "\n",
    "    async def generate_visualization_node(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Calls the LIDA tool logic to generate a visualization.\n",
    "        \"\"\"\n",
    "        print(\"Executing generate_visualization_node\")\n",
    "        question = state['question']\n",
    "        raw_data = state.get('visualization_data') # Data should be populated by fetch_context\n",
    "        viz_request = state.get('visualization_request')\n",
    "\n",
    "        if not raw_data:\n",
    "            state['final_answer'] = \"I couldn't find enough data to create a visualization for that.\"\n",
    "            state['visualization_error'] = \"No data context available for visualization.\"\n",
    "            return state\n",
    "\n",
    "        try:\n",
    "            # Transform data for LIDA (list of dicts to DataFrame)\n",
    "            # Ensure you have pandas installed: pip install pandas\n",
    "            df = data_to_dataframe(raw_data)\n",
    "\n",
    "            # Initialize LIDA (requires an API key, e.g., OpenAI or Google Generative AI)\n",
    "            # Ensure you have lida installed: pip install lida\n",
    "            # and the necessary text generation library (e.g., pip install openai)\n",
    "            from lida import Manager, TextGenerationConfig, DataConfig\n",
    "            # from Constants import Constants # Assuming Constants has get_google_ai_key or similar\n",
    "\n",
    "            # Replace with your actual API key retrieval\n",
    "            # lida_api_key = Constants._get_google_ai_key()\n",
    "            # Or set it directly for testing in notebook\n",
    "            lida_api_key = os.environ.get(\"GOOGLE_API_KEY\") # Example using environment variable\n",
    "\n",
    "            if not lida_api_key:\n",
    "                 state['final_answer'] = \"LIDA API key not found. Please set it up.\"\n",
    "                 state['visualization_error'] = \"Missing LIDA API key.\"\n",
    "                 return state\n",
    "\n",
    "            lida = Manager(text_gen_config=TextGenerationConfig(\n",
    "                model=\"gemini-flash\", # Or the model you intend to use\n",
    "                api_key=lida_api_key\n",
    "            ))\n",
    "\n",
    "            # Summarize the data using LIDA\n",
    "            # LIDA's summarize takes a file path or pandas DataFrame\n",
    "            # If using DataFrame directly, you might need DataConfig\n",
    "            data_summary = lida.summarize(data_frame=df, data_config=DataConfig())\n",
    "            print(f\"LIDA Data Summary: {data_summary}\")\n",
    "\n",
    "            # Use the user's query or the parsed viz_request for visualization generation\n",
    "            visualization_goal = question # You might refine this based on viz_request details\n",
    "\n",
    "            # Generate and execute visualization code specs\n",
    "            viz_code_specs = lida.generate_viz(\n",
    "                summary=data_summary,\n",
    "                goal=visualization_goal, # Use user query as goal\n",
    "                textgen_config=TextGenerationConfig(model=\"gemini-flash\", api_key=lida_api_key),\n",
    "                library=\"altair\" # Specify a library LIDA should use (altair is common for vega-lite)\n",
    "                # Ensure you have altair installed: pip install altair\n",
    "            )\n",
    "            print(f\"LIDA Generated Viz Code Specs: {viz_code_specs}\")\n",
    "\n",
    "            if viz_code_specs:\n",
    "                 # Execute the visualization code\n",
    "                 # LIDA's execute_viz needs the code specs, data, and summary\n",
    "                 # Ensure the data format is what execute_viz expects (likely DataFrame)\n",
    "                 charts = lida.execute_viz(\n",
    "                      code_specs=viz_code_specs[0].code, # Assuming the first spec is relevant and has a 'code' field\n",
    "                      data=df,\n",
    "                      summary=data_summary,\n",
    "                      library=\"altair\"\n",
    "                  )\n",
    "                 print(f\"LIDA Executed Charts: {charts}\")\n",
    "\n",
    "                 if charts:\n",
    "                      # Assuming charts is a list of dictionaries, each with 'spec' and 'raster' (image)\n",
    "                      # Let's return the Vega-Lite spec if available\n",
    "                     first_chart = charts[0]\n",
    "                     if 'spec' in first_chart:\n",
    "                          state['visualization_output'] = {\n",
    "                              \"type\": \"vega-lite\",\n",
    "                              \"spec\": first_chart['spec'] # Vega-Lite JSON specification\n",
    "                          }\n",
    "                          state['final_answer'] = \"Here is the visualization you requested:\" # A text intro\n",
    "                     elif 'raster' in first_chart:\n",
    "                          # If no spec but image is available, return the image (base64)\n",
    "                          # LIDA might return base64 directly or a path. Check LIDA docs/output.\n",
    "                          # Assume it's base64 for now.\n",
    "                         state['visualization_output'] = {\n",
    "                             \"type\": \"image\",\n",
    "                             \"data\": first_chart['raster'] # Base64 encoded image data\n",
    "                         }\n",
    "                         state['final_answer'] = \"Here is the visualization you requested:\" # A text intro\n",
    "                     else:\n",
    "                         state['final_answer'] = \"I generated a visualization, but I couldn't format it correctly.\"\n",
    "                         state['visualization_error'] = \"LIDA execution returned unexpected format.\"\n",
    "\n",
    "                 else:\n",
    "                      state['final_answer'] = \"I was able to generate visualization code, but executing it failed.\"\n",
    "                      state['visualization_error'] = \"LIDA execute_viz failed.\"\n",
    "\n",
    "            else:\n",
    "                state['final_answer'] = \"I understood you wanted a visualization, but I couldn't generate a suitable one based on the data and your request.\"\n",
    "                state['visualization_error'] = \"LIDA generate_viz failed.\"\n",
    "\n",
    "        except ImportError as e:\n",
    "             state['final_answer'] = f\"Missing required library: {e}. Please install it (e.g., pip install lida pandas altair).\"\n",
    "             state['visualization_error'] = f\"Import error: {e}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error during LIDA visualization generation: {e}\")\n",
    "            state['final_answer'] = \"An error occurred while trying to generate the visualization.\"\n",
    "            state['visualization_error'] = f\"LIDA generation error: {e}\"\n",
    "\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1888d",
   "metadata": {},
   "source": [
    "## Fetch Context and Raw Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Modify fetch_context to store raw results if visualization is requested\n",
    "    async def fetch_context(self, state: State) -> State:\n",
    "        print(\"Executing fetch_context\")\n",
    "        question = state['question']\n",
    "        history = state.get('history', '')\n",
    "        viz_request = state.get('visualization_request') # Check if viz was requested\n",
    "\n",
    "        prompt = f\"Question : {question},\\n Chat History: {state.get('history','')}\"\n",
    "\n",
    "        try:\n",
    "            # Use the tool_model to decide which search tool to use\n",
    "            tool_decision_prompt = f\"\"\"\n",
    "            Question Instructions\\\\n\n",
    "                Inputs:\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Chat History: {history}\n",
    "\n",
    "                Task: Your task is to generate an effective search query and determine the appropriate tool (vector_search_tool or parameter_search_tool) to retrieve the necessary information from the database. Consider both the question and the chat history (if relevant) when constructing your query and selecting the tool.\n",
    "                If the question asks for specific attributes or uses precise terms, favor `parameter_search_tool`. If it's a more general or conceptual query, favor `vector_search_tool`.\n",
    "                Also, determine if the user is asking for a *count* of entities. If so, set the `number` argument for `vector_search_tool` to true.\n",
    "\n",
    "                Output: Return the tool call in the specified format.\n",
    "\n",
    "                End of Instructions\\\\n\n",
    "            \"\"\"\n",
    "            tool_d = await self.tool_model.ainvoke(tool_decision_prompt)\n",
    "            tool_call = tool_d.additional_kwargs.get(\"function_call\", {})\n",
    "\n",
    "            if tool_call:\n",
    "                func_name = tool_call.get(\"name\", \"\")\n",
    "                # In a real scenario, you'd parse and use func_args\n",
    "                # For this mock, we'll just use a simple query string\n",
    "                tool_query = question # Use the original question as a simple query\n",
    "\n",
    "                if func_name in self.tools:\n",
    "                    print(f\"Invoking tool: {func_name}\")\n",
    "                    # Invoke the tool and get raw output\n",
    "                    raw_output = self.tools[func_name].invoke(tool_query) # Use .invoke for sync mock tools\n",
    "\n",
    "                    if viz_request and raw_output and isinstance(raw_output, list):\n",
    "                        # If viz is requested and tool returned structured data (e.g., from parameter_search)\n",
    "                        # Store this raw data for the visualization tool\n",
    "                        state['visualization_data'] = raw_output\n",
    "                        state['context'] = \"Data fetched for visualization.\" # Indicate data was fetched\n",
    "                        state['summarized_context'] = \"\" # Clear summarized context for viz path\n",
    "                    else:\n",
    "                        # If no viz request or tool didn't return structured data for viz,\n",
    "                        # convert raw output to a string for summarization\n",
    "                        context_string = json.dumps(raw_output) # Convert raw output to string\n",
    "                        state['context'] = context_string\n",
    "                        state['summarized_context'] = \"\" # Summarization will happen in the next node\n",
    "\n",
    "                else:\n",
    "                    state['context'] = \"Could not identify relevant tool.\"\n",
    "                    state['summarized_context'] = \"\"\n",
    "                    state['final_answer'] = \"I couldn't find the right tool to fetch data for that.\"\n",
    "\n",
    "            else:\n",
    "                 state['context'] = \"Could not determine tool to call.\"\n",
    "                 state['summarized_context'] = \"\"\n",
    "                 state['final_answer'] = \"I couldn't determine how to fetch data for your request.\"\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fetch_context: {e}\")\n",
    "            state['context'] = f\"Error fetching data: {e}\"\n",
    "            state['summarized_context'] = \"\"\n",
    "            state['final_answer'] = \"An error occurred while fetching data.\"\n",
    "            # Optionally, re-raise the exception if you want the graph to stop\n",
    "            # raise e\n",
    "\n",
    "        return state\n",
    "\n",
    "    async def summarize_context(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Summarizes the fetched context for the text workflow.\n",
    "        This node is skipped if a visualization is requested.\n",
    "        \"\"\"\n",
    "        print(\"Executing summarize_context\")\n",
    "        context = state.get('context')\n",
    "        if context and not state.get('visualization_request'):\n",
    "             summary_prompt = f\"\"\"Summarize the following context for a chatbot response:\n",
    "             {context}\n",
    "             \"\"\"\n",
    "             try:\n",
    "                 summary = await self.summarize_model.ainvoke(summary_prompt)\n",
    "                 state['summarized_context'] = summary.content\n",
    "             except Exception as e:\n",
    "                 print(f\"Error in summarize_context: {e}\")\n",
    "                 state['summarized_context'] = \"Error summarizing context.\"\n",
    "                 state['final_answer'] = \"An error occurred while summarizing the information.\"\n",
    "                 # Optionally, re-raise the exception\n",
    "                 # raise e\n",
    "        elif state.get('visualization_request'):\n",
    "             print(\"Skipping summarization as visualization is requested.\")\n",
    "             state['summarized_context'] = \"\" # Ensure empty if skipped\n",
    "\n",
    "        return state\n",
    "\n",
    "    async def generate_answer(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Generates the final text answer based on the summarized context.\n",
    "        This node is skipped if a visualization is requested (unless you modify routing).\n",
    "        \"\"\"\n",
    "        print(\"Executing generate_answer\")\n",
    "        question = state['question']\n",
    "        summarized_context = state.get('summarized_context')\n",
    "        history = state.get('history', '')\n",
    "\n",
    "        if summarized_context and not state.get('visualization_output'):\n",
    "             answer_prompt = f\"\"\"Based on the following summarized context and chat history, answer the user's question:\n",
    "\n",
    "             Question: {question}\n",
    "             Chat History: {history}\n",
    "             Summarized Context: {summarized_context}\n",
    "\n",
    "             Provide a concise and helpful answer.\n",
    "             \"\"\"\n",
    "             try:\n",
    "                 answer = await self.answer_model.ainvoke(answer_prompt)\n",
    "                 state['final_answer'] = answer.content\n",
    "             except Exception as e:\n",
    "                 print(f\"Error in generate_answer: {e}\")\n",
    "                 state['final_answer'] = \"An error occurred while generating the answer.\"\n",
    "                 # Optionally, re-raise the exception\n",
    "                 # raise e\n",
    "        elif state.get('visualization_output') and state.get('final_answer') and \"Here is the visualization\" in state.get('final_answer'):\n",
    "            # If visualization was successful, generate_visualization_node already set final_answer\n",
    "            print(\"Skipping generate_answer as visualization output exists.\")\n",
    "            pass # Keep the final_answer set by generate_visualization_node\n",
    "        else:\n",
    "             # This case might be hit if neither summary nor viz output is ready, or an error occurred earlier\n",
    "             if not state.get('final_answer'): # Avoid overwriting a previous error message\n",
    "                 state['final_answer'] = \"Could not generate a response based on the available information.\"\n",
    "\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65aaa80",
   "metadata": {},
   "source": [
    "## Usage in Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6790e40",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(raw_data)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# You will need to replace this with your actual LLM initialization\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# from langchain_core.language_models import BaseChatModel\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# from langchain_core.output_parsers import JsonOutputParser\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# from langchain_core.runnables import RunnablePassthrough\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Mock LLM classes for decision making and response generation\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMockDecisionLLM\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "# --- How to use in your Jupyter Notebook ---\n",
    "\n",
    "# 1. Initialize the GraphBuilder\n",
    "graph_builder = GraphBuilder()\n",
    "\n",
    "# 2. Create the workflow\n",
    "workflow = graph_builder.create_workflow()\n",
    "\n",
    "# 3. Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# 4. Run the graph with an initial state\n",
    "# You'll need to provide the initial state dictionary\n",
    "# Example state for a text query:\n",
    "initial_state_text = {\n",
    "    'messages': [],\n",
    "    'history': '',\n",
    "    'question': 'Tell me about the organizations in the data.',\n",
    "    'should_split': False,\n",
    "    'split_questions': [],\n",
    "    'needs_history': False,\n",
    "    'needs_context': False,\n",
    "    'context': '',\n",
    "    'summarized_context': '',\n",
    "    'final_answer': '',\n",
    "    'visualization_request': None,\n",
    "    'visualization_data': None,\n",
    "    'visualization_output': None,\n",
    "    'visualization_error': None\n",
    "}\n",
    "\n",
    "# Example state for a visualization query:\n",
    "initial_state_viz = {\n",
    "    'messages': [],\n",
    "    'history': '',\n",
    "    'question': 'Show me a bar chart of organizations by category.',\n",
    "    'should_split': False,\n",
    "    'split_questions': [],\n",
    "    'needs_history': False,\n",
    "    'needs_context': False,\n",
    "    'context': '',\n",
    "    'summarized_context': '',\n",
    "    'final_answer': '',\n",
    "    'visualization_request': None, # This will be set by decide_next_step\n",
    "    'visualization_data': None,     # This will be populated by fetch_context\n",
    "    'visualization_output': None,   # This will be populated by generate_visualization\n",
    "    'visualization_error': None\n",
    "}\n",
    "\n",
    "# To run the graph (use await if in an async environment like certain parts of Jupyter)\n",
    "# For a simple test in a synchronous notebook cell, you might need to use asyncio.run()\n",
    "import asyncio\n",
    "\n",
    "async def run_graph(state):\n",
    "    # The graph runs until it reaches END\n",
    "    result = await app.ainvoke(state)\n",
    "    return result\n",
    "\n",
    "# Example usage (run one at a time)\n",
    "# print(\"Running text query...\")\n",
    "# text_result = await run_graph(initial_state_text)\n",
    "# print(\"\\n--- Text Result ---\")\n",
    "# print(text_result)\n",
    "\n",
    "print(\"\\nRunning visualization query...\")\n",
    "viz_result = await run_graph(initial_state_viz)\n",
    "print(\"\\n--- Visualization Result ---\")\n",
    "print(viz_result)\n",
    "\n",
    "# To view the graph structure (requires graphviz: pip install graphviz)\n",
    "# import graphviz\n",
    "# graphviz.Source(app.get_graph().draw()).render('graph', view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternotebooks-ej2AvU7G-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
